{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vC-94JKSk-G5"
      },
      "outputs": [],
      "source": [
        "# tutorial on pytorch and transformers.\n",
        "# https://towardsdatascience.com/how-to-train-bert-aaad00533168"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eco-a6Fyoy9E",
        "outputId": "098f5f80-081a-4180-b494-e6af37b8e85b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n"
          ]
        }
      ],
      "source": [
        "# install packages and modules.\n",
        "# ! pip install tokenizers===0.9.3\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRjiZ5jClNuE",
        "outputId": "5bffe1b7-8a95-4762-e203-b00be164b4c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "from transformers import BertTokenizer, BertForPreTraining, AdamW\n",
        "import torch\n",
        "# from google.colab import drive\n",
        "# \n",
        "# drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "MM9QMjZRtgDT",
        "outputId": "1fcf40ed-2807-45d0-d8a9-321905a9c639"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9b5d4433ea94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"drive/MyDrive/datasets/clean.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/MyDrive/datasets/clean.txt'"
          ]
        }
      ],
      "source": [
        "# load data\n",
        "text = \"\"\n",
        "with open(\"./clean.txt\", \"r\") as data_obj:\n",
        "  text = data_obj.read().split('\\n')\n",
        "\n",
        "text[ : 3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F31-qimOryjV"
      },
      "outputs": [],
      "source": [
        "# define objects\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgT0IEswuPn9"
      },
      "outputs": [],
      "source": [
        "# next sentence prediction\n",
        "import random\n",
        "\n",
        "# \n",
        "bag = [item for sentence in text for item in sentence.split('.') if item != '']\n",
        "bag_size = len(bag)\n",
        "\n",
        "# create 50/50 random and non-random nsp sentences\n",
        "sentence_a = []\n",
        "sentence_b = []\n",
        "label = []\n",
        "\n",
        "for paragraph in text:\n",
        "  sentences = [sentence for sentence in paragraph.split('.') if sentence != '']\n",
        "  num_sentences = len(sentences)\n",
        "  \n",
        "  if num_sentences > 1:\n",
        "    start = random.randint(0, num_sentences - 2 )\n",
        "    # 50/50 whether is IsNextSentence or NotNextSentence\n",
        "    if random.random() >= .5:\n",
        "      # this is IsNextSentence\n",
        "      sentence_a.append( sentences[start] )\n",
        "      sentence_b.append( sentences[start+1] )\n",
        "      label.append(0)\n",
        "    else:\n",
        "      index = random.randint(0, bag_size-1)\n",
        "      sentence_a.append( sentences[start] )\n",
        "      sentence_b.append( bag[index] )\n",
        "      label.append(1)\n",
        "\n",
        "# len(sentence_a), len(sentence_b)\n",
        "\n",
        "inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "# ['input_ids', 'token_type_ids', 'attention_mask']\n",
        "\n",
        "# add a label to our input, convert labels into tensor and transpose it\n",
        "inputs['next_sentence_label'] = torch.LongTensor([label]).T\n",
        "\n",
        "# clone input_ids to create masked language model label\n",
        "inputs['label'] = inputs.input_ids.detach().clone()\n",
        "\n",
        "# inputs.keys()\n",
        "\n",
        "# create random array of floats with equal dimensions to input_ids tensor\n",
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "\n",
        "# create mask array\n",
        "# by adding the additional logic when creating mask_arr — we are ensuring \n",
        "# that we don’t mask any special tokens — such as CLS (101), SEP (102), and PAD (0) tokens.\n",
        "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)\n",
        "\n",
        "selection = []\n",
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    selection.append( torch.flatten(mask_arr[i].nonzero()).tolist() )\n",
        "\n",
        "for i in range(inputs.input_ids.shape[0]):\n",
        "    inputs.input_ids[i, selection[i]] = 103\n",
        "\n",
        "# Then apply these indices to each row in input_ids, assigning\n",
        "# each value at these indices a value of 103."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq1RAa09ChLK"
      },
      "outputs": [],
      "source": [
        "# create a pytorch dataset from our data\n",
        "class OutDataset(torch.utils.data.Dataset):\n",
        "  def __init__( self, encodings ):\n",
        "    self.encodings = encodings\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "  def __len__(self):\n",
        "    return len(self.encodings.input_ids)\n",
        "\n",
        "# create dataset object\n",
        "dataset = OutDataset(inputs)\n",
        "\n",
        "# define and initialise the data loader\n",
        "# The dataloader expects the __len__ method for checking the \n",
        "# total number of samples within our dataset, \n",
        "# and the __getitem__ method for extracting samples.\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "802koaTZ0ofx"
      },
      "outputs": [],
      "source": [
        "# set up for training.\n",
        "from tqdm import tqdm # use for loading.\n",
        "\n",
        "epochs = 2\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "for epoch in range( epochs):\n",
        "  # setup loop with TQDM and dataloader\n",
        "  loop = tqdm(loader, leave=True)\n",
        "  for batch in loop:\n",
        "    # initialize calculated gradients (from prev step)\n",
        "    optim.zero_grad()\n",
        "    # pull all tensor batches required for training\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    token_type_ids = batch['token_type_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    next_sentence_label = batch['next_sentence_label'].to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "    # process\n",
        "\n",
        "    outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                        token_type_ids=token_type_ids,\n",
        "                        next_sentence_label=next_sentence_label,\n",
        "                        labels=labels)\n",
        "    \n",
        "    # extract loss\n",
        "    loss = outputs.loss\n",
        "    # calculate loss for every parameter that needs grad update\n",
        "    loss.backward()\n",
        "    # update parameters\n",
        "    optim.step()\n",
        "    # print relevant info to progress bar\n",
        "    loop.set_description(f'Epoch {epoch}')\n",
        "    loop.set_postfix(loss=loss.item())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "envs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7 (v3.10.7:6cc6b13308, Sep  5 2022, 14:02:52) [Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "e5c230bf2de97ccb5c631b8b57538711e49af5d79728a8b0f721e6df9702d6f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
